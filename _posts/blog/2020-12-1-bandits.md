# Multi Armed Bandits
### Arhum Zafar / December 2020

The multi armed bandit problem can be used to display the exploration vs exploitation paradox. This introduces the bandit problem and how one can solve it using different methods. 


## Table of Contents
1. [Exploration vs Exploitation](#exploration)
2. [Multi-Armed Bandits](#multiarmedbandit)
3. [Defining the Bandit experiment](#define)
4. [Bayesian Significance and the Bernoulli Distribution](#bb)
5. [Maximizing Reward and Minimizing Regret](#max)
6. [Going Further](#last)
<br>

## What is Exploration and Exploitation? <a name="exploration"></a>
We see the battle between exploration and exploitation in almost every day of our lives. For example, let's say that your favorite ice cream shop is down the 
block. If you go there often, you'll always be sure of what to get, but you'll miss the possibility of discovering a better place to have ice cream. On the other
hand, if you try new places all the time, you will very likely find yourself eating unpleasant ice cream every now and then (*Come on now, don't tell me you have the ability to choose the best item on the menu, even when it's your first time*).
<br>
<br>
![jpg](../../images/kopps.jpg)
<br>
<br> 

The given scenario is a prime example of what we call the 
**exploration-exploitation trade-off** : a well-known problem that occurs in scenarios where a learning system (or a human) has to repeatedly make a choice with uncertain pay-offs (like bad ice cream ðŸ˜‰ ). Furthermore, the main dilemma for a decision making system that only has incomplete knowledge of an environment is whether to repeat decisions that have worked well so far (**Exploit**), or to make new decisions in an attempt to reap better rewards (**Explore**).
<br>
<br>

## What are Multi-Armed Bandits? <a name="multiarmedbandit"></a>
Problems surrounding multi-armed bandits are problems that thoroughly demonstrate the exploration vs exploitation tradeoff. Another example, lets say that you're sitting in a casino, in front of several slot machines. Each machine has an unknown probability of the chances that you win the reward at one play -- talk about a high roller scenario. My question for you is: 
*What is the best method that you could take to win the largest rewards?*
<br>
<br>
![jpg](../../images/roller.jpg)
<br>
<br>
One possible approach is to commit to one machine and play it over and over again, to the point where
you would reach a point where you would estimate the "true" win probability; this is proven by the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). Despite how fun this method may be, it's quite wasteful and grants no guarantee towards the best long-term reward.
<br>
In a [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) (MAB) problem, we try to maximize the gain over time by gambling on bandits (*slot machines*) have have different expected outcomes. The concept itself is similar to [A/B Testing](https://en.wikipedia.org/wiki/A/B_testing), something that many of you have experienced as a result of marketing emails, newsletter sign-ups, and more. 
<br>
<br>

## Defining the Bandit <a name="define"></a>

In the example below, we'll simulate 3 bandits, each with an underlying probability of winning stored in the variable `prob_bandits`. Below, the `choose(i)` nethod will choose bandit **i** and randomly decide if it is a win (1), or not (0), based on probability.

```python
import scipy 
from scipy import stats
import matplotlib.pyplot as plt
import numpy as np

# Define the multi-armed bandits
nb_bandits = 3  # Number of bandits
# True probability of winning for each bandit
prob_bandits = [0.45, 0.55, 0.60]


def choose(i):
    """Pull arm of bandit with index `i` and return 1 if win, 
    else return 0."""
    if np.random.rand() < prob_bandits[i]:
        return 1
    else:
        return 0
``` 
<br>
<br>

## Bayesian Significance and the Bernoulli Distribution <a name="bb"></a>
Choosing a specific bandit will result in a win of a certain probability -- the higher the probability, the more likely choosing the arm of the bandit will result in a win. However, we don't know this probability, so we will have to model it against our observations of a certain bandit winning or losing. We will model this probability as **Î¸**. Thus, based off of observing an outcome, x, we can now model this distribution as **P( Î¸ | x )**. Using [Bayes rule](https://en.wikipedia.org/wiki/Bayes%27_theorem), we can now write the relationship as: <br>

 ![png](../../images/form.png)

 <br>
 <br>
 P( Î¸ | x ) is what we call the posterior distribution of Î¸ after obsvering x. Going forward, we'll be able to find this via the likelihood 
 and the prior P( Î¸ )
